{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   longitude   latitude              clc_quartier             clc_secteur  \\\n",
      "0   3.293264  49.840500  Quartier du Centre-Ville             Quai Gayant   \n",
      "1   3.273380  49.861409    Quartier du Vermandois              Stade Cepy   \n",
      "2   3.289068  49.844513  Quartier du Centre-Ville   Rue Villebois Mareuil   \n",
      "3   3.302387  49.861778      Quartier de l'Europe  Square des Marronniers   \n",
      "4   3.304047  49.858446      Quartier de l'Europe           Avenue Buffon   \n",
      "\n",
      "   haut_tot  haut_tronc  tronc_diam fk_arb_etat fk_stadedev     fk_port  \\\n",
      "0       6.0         2.0        37.0    EN PLACE       Jeune  semi libre   \n",
      "1      13.0         1.0       160.0    EN PLACE      Adulte  semi libre   \n",
      "2      12.0         3.0       116.0    REMPLACÉ      Adulte  semi libre   \n",
      "3      16.0         3.0       150.0    EN PLACE      Adulte  semi libre   \n",
      "4       5.0         2.0       170.0    Essouché      Adulte      réduit   \n",
      "\n",
      "  fk_pied fk_situation fk_revetement  age_estim  fk_prec_estim  clc_nbr_diag  \\\n",
      "0   gazon   Alignement           Non       15.0            5.0           0.0   \n",
      "1   gazon       Groupe           Non       50.0           10.0           0.0   \n",
      "2   gazon   Alignement           Non       30.0           10.0           0.0   \n",
      "3   gazon       Groupe           Non       50.0            2.0           0.0   \n",
      "4   gazon        Isolé           Non       40.0            2.0           0.0   \n",
      "\n",
      "  fk_nomtech villeca feuillage remarquable  \n",
      "0     QUERUB   VILLE   Feuillu         Non  \n",
      "1  PINNIGnig   VILLE  Conifère         Non  \n",
      "2     ACEPSE   VILLE   Feuillu         Non  \n",
      "3     ACEPLA   VILLE   Feuillu         Non  \n",
      "4     SALBAB   VILLE   Feuillu         Non  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#recupere les données de Data_Arbre.csv\n",
    "data = pd.read_csv(\"Data_Arbre.csv\")\n",
    "\n",
    "print(data.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'correlation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m correlation_age_estim \u001b[38;5;241m=\u001b[39m \u001b[43mcorrelation\u001b[49m\u001b[38;5;241m.\u001b[39mloc[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage_estim\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      3\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(correlation_age_estim, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoolwarm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'correlation' is not defined"
     ]
    }
   ],
   "source": [
    "# correlation_age_estim = correlation.loc[['age_estim']]\n",
    "# plt.figure(figsize=(7, 1))\n",
    "# sns.heatmap(correlation_age_estim, annot=True, cmap='coolwarm')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241m.\u001b[39mset_theme()\n\u001b[0;32m      4\u001b[0m colonnes_categorielles \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfk_stadedev\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfk_nomtech\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclc_quartier\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclc_secteur\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfk_arb_etat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfk_port\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfk_pied\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfk_situation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfk_revetement\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvilleca\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeuillage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremarquable\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m rows \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mlen\u001b[39m(colonnes_categorielles)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "# import math\n",
    "# sns.set_theme()\n",
    "\n",
    "# colonnes_categorielles = ['fk_stadedev', 'fk_nomtech', 'clc_quartier', 'clc_secteur', 'fk_arb_etat', 'fk_port', 'fk_pied', 'fk_situation', 'fk_revetement', 'villeca', 'feuillage', 'remarquable']\n",
    "# rows = math.ceil(math.sqrt(len(colonnes_categorielles)))\n",
    "# cols = math.ceil(len(colonnes_categorielles) / rows)\n",
    "# fig, axes = plt.subplots(rows, cols, figsize=(20,14))\n",
    "\n",
    "\n",
    "# if isinstance(axes, np.ndarray):\n",
    "#     axes = axes.flatten()\n",
    "# else:\n",
    "#     axes = [axes]\n",
    "    \n",
    "# for i, (colonne, ax) in enumerate(zip(colonnes_categorielles, axes.flatten())):\n",
    "#     sns.kdeplot(data=data, x='age_estim', hue=colonne, ax= ax)\n",
    "#     ax.set_xlim(0, 200)\n",
    "#     ax.get_legend().remove()\n",
    "#     ax.set_xlabel('')\n",
    "#     ax.set_title(colonne)\n",
    "# for j in range(i + 1, len(axes)):\n",
    "#     axes[j].axis('off')\n",
    "    \n",
    "# plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tronc_diam  haut_tronc  haut_tot  clc_nbr_diag fk_stadedev fk_nomtech\n",
      "0        37.0         2.0       6.0           0.0       Jeune     QUERUB\n",
      "1       160.0         1.0      13.0           0.0      Adulte  PINNIGnig\n",
      "2       116.0         3.0      12.0           0.0      Adulte     ACEPSE\n",
      "3       150.0         3.0      16.0           0.0      Adulte     ACEPLA\n",
      "4       170.0         2.0       5.0           0.0      Adulte     SALBAB\n"
     ]
    }
   ],
   "source": [
    "#sépare les données en deux dataframes, un pour les X et un pour les Y, les X sont la colonne age_estim, et les Y sont les colonnes tronc_diam  + haut_tronc + clc_nbr_diag \n",
    "Y = data[['age_estim']]\n",
    "\n",
    "X = data[['tronc_diam','haut_tronc', 'haut_tot', 'clc_nbr_diag', 'fk_stadedev', 'fk_nomtech']]\n",
    "\n",
    "print(X.head(5))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexm\\AppData\\Local\\Temp\\ipykernel_41864\\1179173926.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['fk_stadedev'] = encoder_stadedev.fit_transform(X[['fk_stadedev']])\n",
      "C:\\Users\\alexm\\AppData\\Local\\Temp\\ipykernel_41864\\1179173926.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['fk_nomtech'] = encoder_nomtech.fit_transform(X[['fk_nomtech']])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "encoder_stadedev = OrdinalEncoder()\n",
    "#encoder permet de transformer les valeurs de la colonne en valeurs numériques (ordonnees)\n",
    "encoder_nomtech = OrdinalEncoder()\n",
    "#label permet de transformer les valeurs de la colonne en valeurs numériques\n",
    "\n",
    "\n",
    "X['fk_stadedev'] = encoder_stadedev.fit_transform(X[['fk_stadedev']])\n",
    "X['fk_nomtech'] = encoder_nomtech.fit_transform(X[['fk_nomtech']])\n",
    "# X['feuillage'] = encoder.fit_transform(X[['feuillage']])\n",
    "# print(X['feuillage'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder permet de transformer les valeurs de la colonne en valeurs numériques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tronc_diam', 'haut_tronc', 'haut_tot', 'clc_nbr_diag', 'fk_stadedev',\n",
      "       'fk_nomtech'],\n",
      "      dtype='object')\n",
      "(7409, 6) (7409, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_X = StandardScaler()\n",
    "scaler_Y = StandardScaler()\n",
    "print(X.columns)\n",
    "\n",
    "X = scaler_X.fit_transform(X)\n",
    "Y = scaler_Y.fit_transform(Y)\n",
    "print(X.shape, Y.shape)\n",
    "# print(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaler réduit remarquablement les scores RMSE et MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.12165495  1.21102921  1.02314578 -0.27980404 -0.56224657  1.5375964 ]\n",
      " [-0.33570685 -0.46555    -0.59694528 -0.27980404 -0.56224657 -1.0182212 ]\n",
      " [ 0.9347426  -1.02440973  0.53711846 -0.27980404 -0.56224657 -0.95878358]\n",
      " ...\n",
      " [-0.08161696  0.09330974  1.83319131 -0.27980404 -0.56224657 -1.28569048]\n",
      " [ 1.44292238 -0.74497987 -0.59694528 -0.27980404 -0.56224657 -0.78047072]\n",
      " [ 0.34186619 -0.46555    -0.43493618 -0.27980404 -0.56224657  0.86892319]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# tableau contenant tous les indices des lignes de data\n",
    "indices = np.arange(data.shape[0])\n",
    "X_train, X_test, Y_train, Y_test, indices_train, indices_test = train_test_split(X, Y, indices, test_size=0.2, random_state=42)\n",
    "#indices_train et indices_test contiennent les indices des lignes de X_train et X_test respectivement\n",
    "\n",
    "# print(\"Indices des lignes dans X_test:\", indices_test)\n",
    "# print(\"Indices des lignes dans X:\", indices)\n",
    "\n",
    "#on réouvre le fichier Data_Arbre.csv pour être sûr de récupérer les données à l'état pur\n",
    "data_test_csv = pd.read_csv(\"Data_Arbre.csv\")\n",
    "\n",
    "#on créé un dataframe contenant toutes les lignes qui ont un indice se trouvant dans indoce test\n",
    "data_test = data_test_csv.iloc[indices_test]\n",
    "\n",
    "#on sauvegarde les données de data_test dans un fichier json Data_Arbre_test.json\n",
    "data_test.to_json('Data_Arbre_test.json', orient='records')\n",
    "\n",
    "print(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search pour RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forest = RandomForestRegressor()\n",
    "\n",
    "# param_grid = {\n",
    "#     'n_estimators': [200,300,400],\n",
    "#     'max_depth' : [10,15,20],\n",
    "#     'min_samples_split': [2,3],\n",
    "#     'min_samples_leaf': [1, 2],\n",
    "#     'max_samples': [0.6, 0.7, 0.8,0.9, 1]\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(estimator=forest, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "# grid_search.fit(X_train, Y_train)\n",
    "# best_model = grid_search.best_estimator_\n",
    "# print('best params :', grid_search.best_params_)\n",
    "\n",
    "\n",
    "# # best params : {'max_depth': 15, 'max_samples': 0.8, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 400}\n",
    "# # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\alexm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\alexm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\alexm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\alexm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\alexm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score : 0.7481679059353971\n",
      "R2 score : 0.8181589076193863\n",
      "RMSE : 0.4260740869260982\n",
      "MAE : 0.25305225751731947\n"
     ]
    }
   ],
   "source": [
    "\n",
    "random_forest = RandomForestRegressor(max_depth= 15, min_samples_leaf= 1, min_samples_split= 2, \n",
    "                                         n_estimators= 400, max_samples=0.8, max_features=\"log2\", random_state=42)\n",
    "\n",
    "random_forest = random_forest.fit(X_train, Y_train)\n",
    "\n",
    "cross_val_score_forest = cross_val_score(random_forest, X_test, Y_test, cv=5).mean()\n",
    "\n",
    "Y_pred = random_forest.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "r2_score_forest = r2_score(Y_test, Y_pred)\n",
    "mae = mean_absolute_error(Y_test, Y_pred)\n",
    "\n",
    "\n",
    "print(\"Cross val score :\", cross_val_score_forest)\n",
    "print(\"R2 score :\", r2_score_forest)\n",
    "print(\"RMSE :\", rmse)\n",
    "print(\"MAE :\", mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#CART : Classification And Regression Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le DecisionTreeRegressor est une implémentation de l'algorithme CART (Classification And Regression Tree) pour les problèmes de régression\n",
    "C'est un arbre qui va diviser l'ensemble des données, en sous-ensemble, et le refaire plusieurs fois\n",
    "\n",
    "Au début, la racine contient l'ensemble des données\n",
    "Pour chaque caractéristique, l'algorithme cherche le meilleur point de division (seuil) qui divise les données en deux sous-ensembles de manière à réduire au maximum l'erreur de régression.\n",
    "L'erreur de régression est souvent mesurée par le MSE (Mean Squared Error) ou le MAE (Mean Absolute Error). Le but est de minimiser cette erreur dans les sous-ensembles qui sont créés.\n",
    "\n",
    "Quand on a trouvé le meilleur point de division, on divise les données en 2 sous-ensemble à partir de ce point.\n",
    "\n",
    "On répète la même méthode sur chaque sous-ensemble\n",
    "\n",
    "Chaque noeud de l'arbre représente une condition sur une caractéristique et un seuil, et chaque feuille représente une prédiction de la valeur cible.\n",
    "\n",
    "On s'arrête quand on a atteint une des conditions d'arrêt (profondeure maximale, nombre d'échantillons dans un noeud inférieur à un seuil minimum, si on ne réduit plus l'erreur de régression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explication des hyperparametres du grid search :\n",
    "\n",
    "max_depth : profondeur maximale de l'arbre, peut réduire le risque de surapprentissage.\n",
    "min_samples_split : Le nombre minimum d'échantillons requis pour diviser un noeud interne.\n",
    "min_samples_leaf : nombre minimum d'échantillons requis pour être à un noeud feuille."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #grid search pour trouver les meilleurs paramètres de decision tree regressor\n",
    "# param_grid = {\n",
    "#     'max_depth' : [7,8,9,10,11,12,13],\n",
    "#     'min_samples_split': [2,3,4,5,6,7,8],\n",
    "#     'min_samples_leaf': [1, 2, 3, 4]\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(estimator=DecisionTreeRegressor(), param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "# grid_search.fit(X_train, Y_train)\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# print('best params :', grid_search.best_params_)\n",
    "# #best params : {'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score : 0.6452069641166899\n",
      "R2 score : 0.7470524890378039\n",
      "RMSE : 0.502521111002659\n",
      "MAE : 0.2976609236037964\n"
     ]
    }
   ],
   "source": [
    "best_model_found = DecisionTreeRegressor(max_depth= 10, min_samples_leaf= 3, min_samples_split= 4, random_state=42)\n",
    "\n",
    "tree = best_model_found\n",
    "tree.fit(X_train, Y_train)\n",
    "\n",
    "cross_val_score_tree = cross_val_score(tree, X_test, Y_test, cv=5).mean()\n",
    "\n",
    "Y_pred = tree.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(Y_test, Y_pred)\n",
    "r2_scoreee = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(\"Cross val score :\", cross_val_score_tree)\n",
    "print(\"R2 score :\", r2_scoreee)\n",
    "print(\"RMSE :\", rmse)\n",
    "print(\"MAE :\", mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "methode 3 : Réseau de neurones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search pour réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search pour trouver les meilleurs paramètres de MLPRegressor\n",
    "# param_grid : dictionnaire contenant les différentes combinaisons de paramètres à tester\n",
    "# 'hidden_layer_sizes' : liste des différentes architectures de couches cachées à tester\n",
    "# 'activation' : liste des différentes fonctions d'activation à tester\n",
    "# 'solver' : liste des différents solveurs à tester\n",
    "# 'alpha' : liste des différents paramètres de régularisation à tester\n",
    "# 'learning_rate' : liste des différents taux d'apprentissage à tester\n",
    "\n",
    "# param_grid={\n",
    "#     'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,100)],\n",
    "#     'activation': ['tanh', 'relu'],\n",
    "#     'solver': ['sgd', 'adam'],\n",
    "#     'alpha': [0.0001, 0.05],\n",
    "#     'learning_rate': ['constant','adaptive'],\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(estimator=MLPRegressor(), param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "# grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# print('best params :', grid_search.best_params_)\n",
    "\n",
    "# #best params : {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1631: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\alexm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1631: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\alexm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1631: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\alexm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1631: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\alexm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1631: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\alexm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1631: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score du modèle de régression linéaire : 0.6929615001559026\n",
      "R2 score du modèle de régression linéaire : 0.7595203432036672\n",
      "RMSE du modèle de régression linéaire : 0.4899799148155968\n",
      "MAE du modèle de régression linéaire : 0.32259556789891336\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_model_found = MLPRegressor(hidden_layer_sizes=(24, 48), activation='relu', alpha=0.05, learning_rate='adaptive', solver='adam', max_iter=1000)\n",
    "mlp = best_model_found\n",
    "mlp.fit(X_train, Y_train)\n",
    "\n",
    "cross_val_score_mlp = cross_val_score(mlp, X_test, Y_test, cv=5).mean()\n",
    "\n",
    "Y_pred = mlp.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(Y_test, Y_pred)\n",
    "r2_score_mlp = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(\"Cross val score du modèle de régression linéaire :\", cross_val_score_mlp)\n",
    "print(\"R2 score du modèle de régression linéaire :\", r2_score_mlp)\n",
    "print(\"RMSE du modèle de régression linéaire :\", rmse)\n",
    "print(\"MAE du modèle de régression linéaire :\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #grid search pour trouver les meilleurs paramètres de KNeighborsRegressor\n",
    "# param_grid = {\n",
    "#     'n_neighbors': [9,10,11,12,13,14,15],\n",
    "#     'weights': ['uniform', 'distance'],\n",
    "#     'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "#     'leaf_size': [10,20,30,40,50]\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(estimator=KNeighborsRegressor(), param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "# grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# print('best params :', grid_search.best_params_)\n",
    "\n",
    "# #best params : {'algorithm': 'ball_tree', 'leaf_size': 20, 'n_neighbors': 10, 'weights': 'distance'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score : 0.7085339410672501\n",
      "R2 score : 0.7848117216516649\n",
      "RMSE : 0.46349858389209364\n",
      "MAE : 0.2578538355544143\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_model_found = KNeighborsRegressor(n_neighbors=10, weights='distance', algorithm='ball_tree', leaf_size=20)\n",
    "\n",
    "knn = best_model_found\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "cross_val_score_knn = cross_val_score(knn, X_test, Y_test, cv=5).mean()\n",
    "\n",
    "Y_pred = knn.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(Y_test, Y_pred)\n",
    "r2_score_knn = r2_score(Y_test, Y_pred)\n",
    "\n",
    "\n",
    "print(\"Cross val score :\", cross_val_score_knn)\n",
    "print(\"R2 score :\", r2_score_knn)\n",
    "print(\"RMSE :\", rmse)\n",
    "print(\"MAE :\", mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modele 5 : HistGrandiantBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search pour HistGrandientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #grid search pour trouver les meilleurs paramètres de HistGradientBoostingRegressor\n",
    "\n",
    "# param_grid = {\n",
    "#     'max_iter': [200,300,400,500],\n",
    "#     'max_leaf_nodes': [31, 41, 51],\n",
    "#     'learning_rate': [0.1, 0.2, 0.3],\n",
    "#     'max_depth': [10, 15, 20]\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(estimator=HistGradientBoostingRegressor(), param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "# grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# print('best params :', grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:1310: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\alexm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\alexm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n",
      "C:\\Users\\alexm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:1310: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\alexm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:1310: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\alexm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:1310: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\alexm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:1310: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\alexm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:1310: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score : 0.7263892763502553\n",
      "R2 score : 0.8079655948978623\n",
      "RMSE : 0.43785330109940146\n",
      "MAE : 0.26822405515355563\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hist = HistGradientBoostingRegressor(learning_rate= 0.1, max_depth = 15, max_iter = 300, max_leaf_nodes = 31, random_state=42)\n",
    "hist.fit(X_train, Y_train)\n",
    "\n",
    "cross_val_score_hist = cross_val_score(hist, X_test, Y_test, cv=5).mean()\n",
    "\n",
    "Y_pred = hist.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(Y_test, Y_pred)\n",
    "r2_score_hist = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(\"Cross val score :\", cross_val_score_hist)\n",
    "print(\"R2 score :\", r2_score_hist)\n",
    "print(\"RMSE :\", rmse)\n",
    "print(\"MAE :\", mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On met les modèles en .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "dict_pickle = {\n",
    "    'encoder_stadedev' : encoder_stadedev,\n",
    "    'encoder_nomtech' : encoder_nomtech,\n",
    "    'scaler_X' : scaler_X,\n",
    "    'scaler_Y' : scaler_Y,\n",
    "    'RandomForest': random_forest,\n",
    "    'DecisionTree': tree,\n",
    "    'MLP': mlp,\n",
    "    'KNN': knn,\n",
    "    'Hist': hist\n",
    "}\n",
    "\n",
    "# Sauvegarder le modèle dans un fichier .pkl\n",
    "with open('dict_modeles.pkl', 'wb') as file:\n",
    "    pickle.dump(dict_pickle, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test d'ouverture de nos fichiers .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score du modèle de régression linéaire : 0.8181589076193863\n",
      "RMSE du modèle de régression linéaire : 0.4260740869260982\n",
      "MAE du modèle de régression linéaire : 0.25305225751731947\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('dict.pkl', 'rb') as file:\n",
    "    pkl_dict = pickle.load(file)\n",
    "\n",
    "\n",
    "model = pkl_dict['RandomForest']\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(Y_test, Y_pred)\n",
    "r2_score_model = r2_score(Y_test, Y_pred)\n",
    "\n",
    "# cross_val_score_hist = cross_val_score(model, X_test, Y_test, cv=5).mean()\n",
    "\n",
    "print(\"R2 score du modèle de régression linéaire :\", r2_score_model)\n",
    "print(\"RMSE du modèle de régression linéaire :\", rmse)\n",
    "print(\"MAE du modèle de régression linéaire :\", mae)\n",
    "# print(\"Cross val score du modèle de régression linéaire :\", cross_val_score_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    #Crée la classe Node, noeud du decision tree\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, variance_reduction=None, value=None):\n",
    "\n",
    "        #variables pour les noeuds de décisions (non feuilles)\n",
    "        self.feature_index = feature_index  #quelle colonne de X le noeud utilise pour split\n",
    "        self.threshold = threshold  #valeur de la colonne à partir de laquelle on split\n",
    "        self.variance_reduction = variance_reduction    #variance reduction du split\n",
    "        self.left = left    #sous-arbre gauche, si valeur < threshold\n",
    "        self.right = right  #sous-arbre droit, si valeur >= threshold\n",
    "\n",
    "        #variable pour les feuilles\n",
    "        self.value = value  #valeur de la feuille (valeur moyenne du Y dans la feuille)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On fait le Decision Tree Regressor from scratch: \n",
    "\n",
    "class DecisionTreeRegressorFromScratch():\n",
    "    def __init__(self, min_samples_split=2, max_depth=10):\n",
    "        #initialisation des paramètres du decision tree\n",
    "        self.root = None     #racine du tree\n",
    "\n",
    "        #hyperparamètres du tree (permettent d'éviter l'overfitting)\n",
    "        self.min_samples_split = min_samples_split  #nombre minimum d'échantillons pour split\n",
    "        self.max_depth = max_depth                  #profondeur maximale du tree\n",
    "\n",
    "    #Fonction pour créer le decision tree\n",
    "    def create_tree(self, data, current_depth):\n",
    "\n",
    "        #on récupère toutes les lignes pour X et Y\n",
    "        #X contient toutes les colonnes sauf la dernière (age_estim)\n",
    "        #Y contient la dernière colonne (age_estim)\n",
    "        X = data[:,:-1]\n",
    "        Y = data[:,-1]\n",
    "\n",
    "        #on récupère le nombre de lignes et de colonnes de X \n",
    "        samples_count, features_count = np.shape(X)\n",
    "        best_split = {}\n",
    "\n",
    "        #si on a plus de taille d'échantillon que le minimum demandé et qu'on n'a pas atteint la profondeur max (critères d'arrêt de la récursivité)\n",
    "        if samples_count >= self.min_samples_split and current_depth <= self.max_depth:\n",
    "\n",
    "            #on trouve le meilleur split pour la data actuelle\n",
    "            best_split = self.get_best_split(data, samples_count, features_count)\n",
    "\n",
    "            #si la variance reduction du split est supérieure à 0 (s'il sert à quelque chose)\n",
    "            if best_split['variance_reduction'] > 0:\n",
    "                \n",
    "                #on split la data en deux parties (gauche : inférieure au seuil et droite : supérieure)\n",
    "                left_subtree = self.create_tree(best_split['left_side'], current_depth+1)\n",
    "                right_subtree = self.create_tree(best_split['right_side'], current_depth+1)\n",
    "\n",
    "                return Node(best_split['feature_index'], best_split['threshold'], left_subtree, right_subtree, best_split['variance_reduction'])\n",
    "\n",
    "        #si on a moins de taille d'échantillon que le minimum demandé ou qu'on a atteint la profondeur max   \n",
    "        #ou si on n'a pas trouvé de split qui améliore la variance, on crée une feuille\n",
    "        #on a atteint une feuille (pas de suite à donner)\n",
    "        return Node(value=np.mean(Y)) # np.mean(Y) = valeur de la feuille\n",
    "    \n",
    "    \n",
    "    #Fonction qui permet de trouver le meilleur split pour un dataset\n",
    "    def get_best_split(self, data, samples_count, features_count):\n",
    "        \n",
    "        #création du dictionnaire contenant les informations du meilleur split\n",
    "        best_split = {'feature_index': None, 'threshold': None, 'left_side': None, 'right_side': None, 'variance_reduction': -np.inf}\n",
    "        #on initialise la variance reduction max à -infini (pour etre sur de trouver un split qui améliore la variance que rien)\n",
    "        max_variance_reduction = -float('inf')\n",
    "\n",
    "        #on parcourt les features\n",
    "        for feature_index in range(features_count):\n",
    "            \n",
    "            #on récupère toutes les valeurs possibles de la feature et on les garde une seule fois (récupère les thresholds possibles)\n",
    "            feature_values = data[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "\n",
    "            #on parcourt les thresholds possibles\n",
    "            for threshold in possible_thresholds:\n",
    "                \n",
    "                #on sépare les données en deux parties (gauche et droite) en fonction de la comparaison au threshold\n",
    "                # Utilisation de np.where pour parcourir le tableau et obtenir les indices où la condition est vraie\n",
    "                left_indices = np.where(feature_values <= threshold)[0]\n",
    "                right_indices = np.where(feature_values > threshold)[0]\n",
    "\n",
    "                # Sélection des parties gauche et droite du dataset en utilisant les indices\n",
    "                left_side = data[left_indices]\n",
    "                right_side = data[right_indices]\n",
    "\n",
    "                # On continue seulement si les deux côtés ne sont pas vides\n",
    "                if len(left_side) > 0 and len(right_side) > 0:\n",
    "\n",
    "                    # Extraction des valeurs de Y pour la data complete, gauche et droite\n",
    "                    y, left_y, right_y = data[:, -1], left_side[:, -1], right_side[:, -1]\n",
    "\n",
    "                    # Calcul de la variance actuelle et des variances des côtés gauche et droit\n",
    "                    current_variance = np.var(y)\n",
    "                    left_y_variance = np.var(left_y)\n",
    "                    right_y_variance = np.var(right_y)\n",
    "\n",
    "                    #poids = nombre d'échantillons dans le côté / nombre total d'échantillons\n",
    "                    #len(left_side) / samples_count est le poids du côté gauche\n",
    "                    #len(right_side) / samples_count est le poids du côté droit\n",
    "                    # Calcul de la réduction de variance\n",
    "                    variance_reduction = current_variance - ((len(left_side) / samples_count) * left_y_variance + (len(right_side) / samples_count) * right_y_variance)\n",
    "\n",
    "                    # Mise à jour de la meilleure réduction de variance et des informations de split si nécessaire\n",
    "                    if variance_reduction > max_variance_reduction:\n",
    "                        \n",
    "                        max_variance_reduction = variance_reduction\n",
    "\n",
    "                        best_split['feature_index'] = feature_index\n",
    "                        best_split['threshold'] = threshold\n",
    "                        best_split['left_side'] = left_side\n",
    "                        best_split['right_side'] = right_side\n",
    "                        best_split['variance_reduction'] = variance_reduction\n",
    "                        \n",
    "        return best_split\n",
    "    \n",
    "    #Fonction pour entrainer le modèle à partir des données X et Y (entrée X, sortie Y)\n",
    "    def fit(self, X, Y):\n",
    "        #on crée le dataset en concaténant X et Y\n",
    "        self.root = self.create_tree(np.concatenate((X,Y), axis=1), 0) \n",
    "\n",
    "    #Fonction pour prédire les valeurs de Y à partir des données X\n",
    "    def predict(self, X):\n",
    "        #liste contenant les prédictions pour chaque ligne de X\n",
    "        predictions = [] \n",
    "\n",
    "        #on parcourt les lignes de X\n",
    "        for x in X:\n",
    "            #on initialise le tree à la racine\n",
    "            tree = self.root\n",
    "\n",
    "            #tant qu'on n'est pas arrivé à une feuille\n",
    "            while tree.value is None:\n",
    "                #on récupère la feature de la ligne x\n",
    "                feature_val = x[tree.feature_index]\n",
    "\n",
    "                #on compare la feature de x au threshold du noeud\n",
    "                if feature_val <= tree.threshold: #si la feature de x est inférieure ou égale au threshold\n",
    "                    tree = tree.left    #on va à gauche\n",
    "                else:\n",
    "                    tree = tree.right   #sinon on va à droite\n",
    "\n",
    "            #notre x a atteint une feuille (il a donc les mêmes critères que l'échantillon de la feuille)\n",
    "            #on ajoute la valeur de la feuille (moyenne des Y des échantillons) comme valeur de prédiction pour notre x\n",
    "            predictions.append(tree.value)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test du CART from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score : 0.7115323392128504\n",
      "RMSE : 0.5366457231383555\n",
      "MAE : 0.28528328655993324\n"
     ]
    }
   ],
   "source": [
    "CARTFromScratch = DecisionTreeRegressorFromScratch()\n",
    "CARTFromScratch.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = CARTFromScratch.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(Y_test, Y_pred)\n",
    "\n",
    "r2_score_CART = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(\"R2 score :\", r2_score_CART)\n",
    "print(\"RMSE :\", rmse)\n",
    "print(\"MAE :\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestRegressorFromScratch():\n",
    "    def __init__(self, n_trees=10, min_samples_split=2, max_depth=5):\n",
    "        self.n_trees = n_trees\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.score_oob = 0\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "\n",
    "        samples_count = len(X)\n",
    "\n",
    "        #liste contenant les prédictions OOB pour chaque ligne de X (initialisés à 0)\n",
    "        oob_predictions = np.zeros(samples_count) \n",
    "        #liste contenant les compteurs de prédictions OOB pour chaque ligne de X (initialisés à 0)\n",
    "        oob_counts = np.zeros(samples_count) \n",
    "        \n",
    "        #on parcourt chaque arbre\n",
    "        for i in range(self.n_trees):\n",
    "            #on crée un échantillon bootstrap\n",
    "            bootstrap_indices = np.random.choice(samples_count, samples_count, replace=True)\n",
    "            X_bootstrap, Y_bootstrap = X[bootstrap_indices], Y[bootstrap_indices]\n",
    "\n",
    "            #on crée un CART\n",
    "            tree = DecisionTreeRegressorFromScratch(self.min_samples_split, self.max_depth)\n",
    "            tree.fit(X_bootstrap, Y_bootstrap)\n",
    "\n",
    "            #on ajoute l'arbre à la liste des arbres\n",
    "            self.trees.append(tree)\n",
    "\n",
    "            #on récupère les indices des lignes qui ne sont pas dans l'échantillon bootstrap\n",
    "            oob_indices = np.setdiff1d(np.arange(samples_count), bootstrap_indices)\n",
    "\n",
    "            if len(oob_indices) > 0:\n",
    "                #on récupère les prédictions OOB pour ces lignes\n",
    "                oob_predictions[oob_indices] += tree.predict(X[oob_indices])\n",
    "                #on incrémente les compteurs de prédictions OOB pour ces lignes\n",
    "                oob_counts[oob_indices] += 1\n",
    "        \n",
    "        mask = oob_counts > 0   #tableau de True ou False selon si le compteur est supérieur à 0\n",
    "        \n",
    "        #on calcule les prédictions OOB finales en divisant par le nombre de compteurs\n",
    "        #oob_predictions[mask] / oob_counts[mask] = moyenne des prédictions OOB pour chaque ligne\n",
    "        self.oob_score_ = r2_score(Y[mask], oob_predictions[mask] / oob_counts[mask]) \n",
    "\n",
    "    def predict(self, X):\n",
    "        #liste contenant les prédictions pour chaque ligne de X (initialisés à 0)\n",
    "        predictions = np.zeros(len(X)) \n",
    "\n",
    "        #on parcourt chaque arbre\n",
    "        for tree in self.trees:\n",
    "            #on récupère les prédictions de l'arbre\n",
    "            tree_predictions = tree.predict(X)\n",
    "            #on ajoute les prédictions de l'arbre à la liste des prédictions\n",
    "            predictions += tree_predictions\n",
    "\n",
    "        #on retourne les prédictions finales en divisant par le nombre d'arbres\n",
    "        return predictions / self.n_trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score : 0.7849397206413262\n",
      "RMSE : 0.4633607135214357\n",
      "MAE : 0.2508191377472613\n",
      "OOB score : 0.818662632156095\n"
     ]
    }
   ],
   "source": [
    "random_forest_from_scratch = RandomForestRegressorFromScratch(n_trees=100, min_samples_split=2, max_depth=15)\n",
    "random_forest_from_scratch.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = random_forest_from_scratch.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(Y_test, Y_pred)\n",
    "\n",
    "r2_score_forest_from_scratch = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(\"R2 score :\", r2_score_forest_from_scratch)\n",
    "print(\"RMSE :\", rmse)\n",
    "print(\"MAE :\", mae)\n",
    "print(\"OOB score :\", random_forest_from_scratch.oob_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
